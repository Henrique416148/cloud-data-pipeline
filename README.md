# â˜ï¸ Cloud Data Pipeline: CoinGecko â†’ BigQuery

![Python](https://img.shields.io/badge/Python-3.9+-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Google Cloud](https://img.shields.io/badge/Google_Cloud-Platform-4285F4?style=for-the-badge&logo=google-cloud&logoColor=white)
![BigQuery](https://img.shields.io/badge/BigQuery-Data_Warehouse-669DF6?style=for-the-badge&logo=google-bigquery&logoColor=white)
![Streamlit](https://img.shields.io/badge/Streamlit-Analytics_Dashboard-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white)

---

## ğŸ“Œ VisÃ£o Geral

Este projeto implementa um **pipeline de Engenharia de Dados end-to-end**, responsÃ¡vel por consumir dados da API pÃºblica da **CoinGecko**, realizar ingestÃ£o em nuvem e estruturar os dados seguindo a **Arquitetura MedalhÃ£o (Bronze, Silver e Gold)** no **Google BigQuery**.

O foco principal do projeto Ã© demonstrar:
- Boas prÃ¡ticas de **engenharia analÃ­tica**
- Uso de **ELT em Data Warehouse**
- **Qualidade e observabilidade dos dados**
- AplicaÃ§Ã£o de **mÃ©tricas financeiras reais** para anÃ¡lise de mercado cripto

---

## ğŸ—ï¸ Arquitetura da SoluÃ§Ã£o

O pipeline segue o padrÃ£o **ELT (Extract, Load, Transform)**, priorizando o BigQuery para transformaÃ§Ãµes pesadas e escalÃ¡veis.

```mermaid
graph LR
    A[API CoinGecko] -->|Extract JSON| B[Python Ingestion]
    B -->|Load Raw Data| C[(BigQuery Bronze)]
    
    subgraph BigQuery Data Warehouse
        C -->|SQL Cleaning & Validation| D[(Silver Layer)]
        D -->|SQL Analytics & Aggregations| E[(Gold Layer)]
    end

    E -->|Consumption| F[Streamlit Dashboard]

    style C fill:#cd7f32,stroke:#333,stroke-width:2px,color:white
    style D fill:#c0c0c0,stroke:#333,stroke-width:2px,color:black
    style E fill:#ffd700,stroke:#333,stroke-width:2px,color:black
```

### 1. Monitoramento de SLA (Data Quality)
![SaÃºde do Pipeline](img/pipeline-health.png)

ğŸ§± Camadas de Dados (Medallion)
ğŸŸ¤ Bronze â€” Raw

Dados exatamente como retornados pela API.

Schema mÃ­nimo e metadados: asset_id, currency, price, price_timestamp, ingestion_timestamp, run_id, source.

if_exists='append' para preservar histÃ³rico.

âšª Silver â€” Curado (View)

DeduplicaÃ§Ã£o (ex.: ROW_NUMBER() por asset_id, currency, price_timestamp ordenando por ingestion_timestamp DESC).

PadronizaÃ§Ã£o (casing, tipos, UTC).

ValidaÃ§Ãµes bÃ¡sicas (price > 0).

ğŸŸ¡ Gold â€” Agregado (Tabela)

AgregaÃ§Ãµes prontas para consumo (diÃ¡rias / horÃ¡rias).

Indicadores financeiros (mÃ©dia mÃ³vel 7d, volatilidade 7d, price close).

Tabela materializada / particionada para performance.

### 2. Entrega da Camada Gold (Business Intelligence)
![VisualizaÃ§Ã£o Gold](img/gold-analysis.png)

ğŸ“ˆ Observabilidade & Data Quality (exemplos)

SLA de ingestÃ£o: meta de 24 coletas/dia. Validar counts por dia.

Checks principais: price NULL, price <= 0, timestamps nulos, lacunas por dia.

Auditoria: run_id e ingestion_timestamp para reprocessamento / investigaÃ§Ã£o.


ğŸ§­ Como Rodar o Projeto (exemplo rÃ¡pido)

PrÃ©-requisitos: Python 3.9+, gcloud CLI autenticado, conta GCP com BigQuery habilitado.


# 1. Clone o repositÃ³rio:

git clone https://github.com/Henrique416148/cloud-data-pipeline.git
cd cloud-data-pipeline

# 2. Crie um ambiente virtual e instale as dependÃªncias:

`python -m venv .venv
source .venv/bin/activate`  
# macOS / Linux
`.venv\Scripts\activate`    
# Windows
`pip install -r requirements.txt`

# 3. Configure credenciais:

Crie um Service Account no GCP com permissÃ£o para BigQuery.

Baixe a chave JSON e adicione em `.gitignore`

Exporte a variÃ¡vel de ambiente:

`export GOOGLE_APPLICATION_CREDENTIALS="/path/to/service_account.json"`

# 4. Rodar a ingestÃ£o local (exemplo):

`python ingest_btc.py`

# 5. Validar dados no BigQuery:

SELECT COUNT(*) FROM `seu-projeto.raw_data.bitcoin_prices_bronze`;

âœ… Boas prÃ¡ticas demonstradas

SeparaÃ§Ã£o de responsabilidades entre ingestÃ£o (Python) e transformaÃ§Ã£o (BigQuery SQL)

Uso de IDs de execuÃ§Ã£o (run_id) e ingestion_timestamp para rastreabilidade

DeduplicaÃ§Ã£o na Silver via ROW_NUMBER() e QUALIFY

DocumentaÃ§Ã£o clara e orientada a produto

ğŸ“‚ RepositÃ³rios & Links

RepositÃ³rio principal: https://github.com/Henrique416148/cloud-data-pipeline

ğŸ‘‹ Sobre Mim
<div align="center"> <h2>Luis Henrique</h2> <h4>Data Engineer | Analytics | Cloud</h4> <p><em>"Transformando dados brutos em insights acionÃ¡veis atravÃ©s de engenharia robusta."</em></p> <p> <a href="https://linkedin.com/in/luis-henrique-dos-ribeiro-991aa8250"> <img src="https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white" alt="LinkedIn"/> </a> </p> </div>

